<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Christophe Website - AI</title><link href="https://tofguerrier.github.io/" rel="alternate"></link><link href="https://tofguerrier.github.io/feeds/ai.atom.xml" rel="self"></link><id>https://tofguerrier.github.io/</id><updated>2024-07-08T20:00:00+01:00</updated><entry><title>Random thoughts on AI - July 2024</title><link href="https://tofguerrier.github.io/articles/random-thoughts-on-ai-july-2024.html" rel="alternate"></link><published>2024-07-08T20:00:00+01:00</published><updated>2024-07-08T20:00:00+01:00</updated><author><name>Christophe</name></author><id>tag:tofguerrier.github.io,2024-07-08:/articles/random-thoughts-on-ai-july-2024.html</id><summary type="html">&lt;p&gt;My thoughts on AI in July 2024&lt;/p&gt;</summary><content type="html">&lt;p&gt;So everyone is talking about AI, either panicking or fantazing about the rise of AI, once again the future is both arriving fast and very slowly at the same time. Here is a capture of my current thought on the subject of AI, actually more questions than answers.
Personally I think this is another bubble like the crypto one, but with a bit more substance. However there is some inherent limitations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Energy
    Training AI, especially with Nvidia GPU is power intensive. There is a rush to equipe data center with the latest GPU, which consumption keeps climbing (400W !!! ). 
    The powering of all the promised AI will be limited by the amount of electricity we decide to contribute to it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training Data
    Soon we will run out of material to train AI, and even without the legal limitations of fair use.
    I do not think, we can train AI on data generated by AI, we will end up with the same problem we have with photocopier.
    On top of this, the current training data, is generated by human worker (not paid very well to say the least) which are used to label picture and videos. There is also the problem that the trained data might be stained by cultural bias (ie who is doing the training).
    Also can Ai expand beyond the training data, for example if we were to train an AI only with data generated before 1850, would AI be able to answer question about general relativity or quantum phenomenon.
    Actually can AI expand and imagine something "new" ? How will it react when faced with something never seen before ?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Diminishing Returns
    Even if we set aside the energy and data issues, it looks like we are going to hit the eternal law of diminishing returns. Idealist thinks we are about to reach an exponential return as we fed more data (which will be limited), a more realistic approach would suggest a linear relationship between amount of training and "intelligence" produced; but I think each percentage increment of "returned intelligence" will require always exponentiallly increasing amount of training data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Most non-engineer believe AI will not be able to replace human in term of emotional interaction, I actually think the opposite. Emotional recognition might be easier to achieve than we think, and human are pretty bad at it ( we are not  telepath ), so AI is likely to surpass humans.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We also have to recognise, we don't understand some serious things about AI and machine learning:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Why is the last LLM so stable ? &lt;/li&gt;
&lt;li&gt;In particular why are they so resilient to overfitting ? In traditional machine learning, overfitting is always an issue, yet here the more parameters, the better the model is.... This is so counter-intuitive.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hallucinations looks to be unavoidable.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verification&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;As we trust more of our systems to AI systems, how will we verify them ? How will we fix them (they are a black box) ? How will we be able to detect hallucinations as they will become more difficult to detect.&lt;/li&gt;
&lt;li&gt;In the end, will we be able to trust the AI system ? Or will we trust them too much ?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having say all that, I also think AI could be really helpful and useful on sone problems.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parsing through scientific paper
    We have (as a human species) generated a vast amount of scientific papers, actually in the last few decades an increasing number. So many that most scientist are not able to read of aware of all of them. AI could help read and parse all those paper, and look for few things, in particular:&lt;/li&gt;
&lt;li&gt;detect fraudulent paper (data manipulation for example, already being used).&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;find common insights and unusual new ideas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parsing through legal documents
    Again the amount of legal paper, makes the use of AI ideal, to detect logical incoherence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="AI"></category><category term="AI"></category></entry></feed>